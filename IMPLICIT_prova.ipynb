{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SJ5UZfcmW2d2dLZJVm3zgZH1bwq25d62","authorship_tag":"ABX9TyNPfLQt6G1l7xk0liaF1VL/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import transformers\n","from transformers import pipeline\n","import pandas as pd\n","import re\n","import spacy"],"metadata":{"id":"ZfKfNp3QICBz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#carico il modello per l'italiano\n","!python -m spacy download it_core_news_lg\n","nlp = spacy.load(\"it_core_news_lg\")"],"metadata":{"id":"eC16tX_apeJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#imposto il modello BERT da usare con la libreria pipeline\n","unmasker = pipeline('fill-mask', model=\"dbmdz/bert-base-italian-xxl-cased\", tokenizer = \"dbmdz/bert-base-italian-xxl-cased\")"],"metadata":{"id":"f5fP9MFPpWTG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prova su 8 frasi rappresentative (1 MASK)"],"metadata":{"id":"ECWfwAWvf4E-"}},{"cell_type":"code","source":["#creo una variabile con le frasi di prova\n","sentences = [\"1) Mi piace bere il vino. Quando bevo [MASK] mi sento bene.\",\n","             \"2) Mi piace bere l'acqua. Quando bevo [MASK] mi sento bene.\",\n","             \"3) Mi piace bere succhi di frutta. Quando bevo [MASK] mi sento bene.\",\n","             \"4) Mi piace bere i succhi di frutta, l'acqua e il vino. Quando bevo [MASK] mi sento bene\",\n","             \"5) Versò la minestra e gliela diede da mangiare. Lui mangiò [MASK] e ringraziò\",\n","             \"6) Versò la minestra e gliela diede da bere. Lui bevve [MASK] e ringraziò\",\n","             \"7) Versò la pozione e gliela diede da bere. Lui bevve [MASK] e ringraziò\",\n","             \"8) Versò la pozione verde e gliela diede da bere. Lui bevve [MASK] verde e ringraziò\"]"],"metadata":{"id":"AGgrUpH7f14y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for sentence in sentences:\n","    noun_predictions = []\n","    #uso l'unmasker per ottenere i primi 10 risultati\n","    results = unmasker(sentence, top_k=10)\n","    #per ogni risultato, applico il pos tagging di SpaCy in modo da isolare solo i nomi\n","    for prediction in results:\n","        doc = nlp(prediction[\"token_str\"])\n","        #aggiungo i nomi candidati alla lista noun_predictions\n","        for token in doc:\n","            if token.pos_ == \"NOUN\":\n","                noun_predictions.append(prediction)\n","    if noun_predictions:\n","        #ordino i candidati in base al prediction score\n","        noun_predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n","        #isolo il candidato con lo score più alto e stampo il punteggio e la frase completa\n","        print(noun_predictions[0][\"sequence\"])\n","    else:\n","        print(\"unknown\")"],"metadata":{"id":"hskuUIXvgBuF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prova su 80 frasi dell'IMPLICIT dataset, primi 4 verbi (2 MASK)"],"metadata":{"id":"ZviAWfWoYIbX"}},{"cell_type":"code","source":["#importo le frasi dal dataset\n","file = pd.read_excel(r'/content/drive/MyDrive/IOcompletion_tesi_agnese_daffara/Dataset/IMPLICIT_results.xlsx')\n","sentences = file['frase'][:80]\n","#for i in sentences:\n","  #print(i)"],"metadata":{"id":"pYYKZTzRnS8U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prova 0:\n","1.   fornire una lista di Determiners\n","2.   iterare su questi per completare il Noun\n","3.   prendere la combinazione con punteggio più alto"],"metadata":{"id":"Fo4BDj5foQHb"}},{"cell_type":"code","source":["output = []\n","\n","for sentence in sentences:\n","    noun_predictions = [] #1\n","    det_predictions = [\"il \", \"lo \", \"la \", \"i \", \"gli \", \"le \", \"\"]\n","    for det in det_predictions:\n","        new_sentence = sentence.replace(\"[MASK]\", det, 1)\n","        results = unmasker(new_sentence, top_k=10)\n","        for prediction in results: #2\n","            doc = nlp(prediction[\"token_str\"])\n","            for token in doc:\n","                if token.pos_ == \"NOUN\":\n","                    noun_predictions.append(prediction)\n","    if noun_predictions: #3\n","        noun_predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n","        output.append(noun_predictions[0][\"token_str\"])\n","        #print(noun_predictions[0][\"token_str\"], noun_predictions[0][\"score\"], noun_predictions[0][\"sequence\"])\n","    else:\n","        output.append(\"unknown\")\n","        #print(\"unknown\")\n","#print(output)"],"metadata":{"id":"x1OYnahAXz4i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["prova 1:\n","1.   riempire il primo [MASK] con un Determiner\n","2.   creare una nuova frase con il primo [MASK] completato\n","3.   completare la nuova frase con il Noun con il punteggio più alto"],"metadata":{"id":"HonV_0_BoWkd"}},{"cell_type":"code","source":["output1 = []\n","for sentence in sentences:\n","    results = unmasker(sentence, top_k=10)\n","    det_predictions = [] #1\n","    for prediction in results[0]:\n","        doc = nlp(prediction[\"token_str\"])\n","        for token in doc:\n","            if token.pos_ == \"DET\":\n","                det_predictions.append(prediction)\n","    if det_predictions:\n","            det_predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n","            determiner = det_predictions[0][\"token_str\"]\n","            new_sentence = sentence.replace(\"[MASK]\", determiner, 1) #2\n","    else:\n","            new_sentence = sentence.replace(\"[MASK]\", \"\", 1)\n","    results_new = unmasker(new_sentence, top_k=10) #3\n","    noun_predictions = []\n","    for prediction in results_new:\n","        doc = nlp(prediction['token_str'])\n","        for token in doc:\n","            if token.pos_ == \"NOUN\":\n","                noun_predictions.append(prediction)\n","    if noun_predictions:\n","        noun_predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n","        output1.append(noun_predictions[0][\"token_str\"])\n","        #print(noun_predictions[0][\"token_str\"], noun_predictions[0][\"score\"], noun_predictions[0][\"sequence\"])\n","    else:\n","        output1.append(\"unknown\")\n","        #print(\"unknown\")\n","#print(output1)"],"metadata":{"id":"t7cBB5sd1pc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["prova 2:\n","1.   riempire il secondo [MASK] ignorando il primo\n","\n"],"metadata":{"id":"NnBcldrXofQ3"}},{"cell_type":"code","source":["output2 = []\n","for sentence in sentences:\n","    results = unmasker(sentence, top_k=10)\n","    noun_predictions = [] #1\n","    for prediction in results[1]:\n","        doc = nlp(prediction[\"token_str\"])\n","        for token in doc:\n","            if token.pos_ == \"NOUN\":\n","                noun_predictions.append(prediction)\n","    if noun_predictions:\n","        noun_predictions.sort(key=lambda x: x[\"score\"], reverse=True)\n","        output2.append(noun_predictions[0][\"token_str\"])\n","        sequence = noun_predictions[0][\"sequence\"]\n","        sequence = re.sub(\"\\[CLS\\]\",\"\", sequence)\n","        sequence = re.sub(\"\\[SEP\\]\",\"\", sequence)\n","        #print(noun_predictions[0][\"token_str\"], noun_predictions[0][\"score\"], sequence)\n","    else:\n","        output2.append(\"unknown\")\n","        #print(\"unknown\")\n","#print(output2)"],"metadata":{"id":"lSi5W31e2USU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Secondo step -- creazione di un **dizionario** e **calcolo** **della** **similarity**"],"metadata":{"id":"nDG5TsBBXOg6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3INmLDQI2z23"},"outputs":[],"source":["#creo tre dizionari che contengono i risultati dei tre esperimenti\n","\n","#carico la lista di annotazioni manuali per le prime 80 frasi\n","predicted = file['obj_1'][:80]\n","\n","#il dizionario associa il risultato del modello all'annotazione manuale creando delle coppie\n","word_dict = {}\n","counter = {}\n","for predicted_word, output_word in zip(predicted, output):\n","    if predicted_word not in word_dict:\n","        word_dict[predicted_word] = output_word\n","    else:\n","        #per evitare sovrapposizioni, aggiungo una numerazione alle parole ripetute\n","        if predicted_word in counter:\n","            counter[predicted_word] += 1\n","        else:\n","            counter[predicted_word] = 1\n","        new_key = f\"{predicted_word}_{counter[predicted_word]}\"\n","        word_dict[new_key] = output_word\n","#print(\"word_dict: \", word_dict)\n","#print(len(word_dict))\n","\n","#ripeto il passaggio sui risultati del secondo esperimento\n","word_dict1 = {}\n","counter = {}\n","for predicted_word, output_word in zip(predicted, output1):\n","    if predicted_word not in word_dict:\n","        word_dict[predicted_word] = output_word\n","    else:\n","        if predicted_word in counter:\n","            counter[predicted_word] += 1\n","        else:\n","            counter[predicted_word] = 1\n","        new_key = f\"{predicted_word}_{counter[predicted_word]}\"\n","        word_dict1[new_key] = output_word\n","#print(\"word_dict1: \", word_dict1)\n","#print(len(word_dict1))\n","\n","#ripeto il passaggio sui risultati del terzo esperimento\n","word_dict2 = {}\n","counter = {}\n","for predicted_word, output_word in zip(predicted, output2):\n","    if predicted_word not in word_dict:\n","        word_dict[predicted_word] = output_word\n","    else:\n","        if predicted_word in counter:\n","            counter[predicted_word] += 1\n","        else:\n","            counter[predicted_word] = 1\n","        new_key = f\"{predicted_word}_{counter[predicted_word]}\"\n","        word_dict2[new_key] = output_word\n","#print(\"word_dict2: \", word_dict2)\n","#print(len(word_dict2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NzuaTn263cmW"},"outputs":[],"source":["#calcolo la media di similarity tra le parole dei tre dizionari e l'annotazione manuale\n","\n","#definisco la funzione per calcolare la similarity tra due parole\n","def calculate_similarity(word1, word2):\n","    #rimuovo sistematicamente il numero di indice aggiunto prima\n","    word1 = re.sub(r\"_\\d+\", \"\", word1)\n","    word2 = re.sub(r\"_\\d+\", \"\", word2)\n","    word1_token = nlp(word1)\n","    word2_token = nlp(word2)\n","    if word1 == \"unknown\" or word2 == \"unknown\":\n","        return 0\n","    if word1_token.has_vector and word2_token.has_vector:\n","        similarity = word1_token.similarity(word2_token)\n","        return similarity\n","    else:\n","      return 0\n","\n","similarity_scores = []\n","for key, value in word_dict.items():\n","    similarity = calculate_similarity(key, value)\n","    similarity_scores.append(similarity)\n","#print(similarity_scores)\n","print(\"media esperimento 0 =\", sum(similarity_scores) / len(similarity_scores))\n","\n","similarity_scores1 = []\n","for key, value in word_dict1.items():\n","    similarity = calculate_similarity(key, value)\n","    similarity_scores1.append(similarity)\n","#print(similarity_scores1)\n","print(\"media esperimento 1 =\", sum(similarity_scores1) / len(similarity_scores1))\n","\n","similarity_scores2 = []\n","for key, value in word_dict2.items():\n","    similarity = calculate_similarity(key, value)\n","    similarity_scores2.append(similarity)\n","#print(similarity_scores2)\n","print(\"media esperimento 2 =\", sum(similarity_scores2) / len(similarity_scores2))"]}]}