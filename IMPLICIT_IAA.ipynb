{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1fP1J1JOtmxKdGOzII6tlDni2ZkiL0GoQ","authorship_tag":"ABX9TyPZ2wOWO1zafEhgH9qdctLe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import spacy\n","import scipy as spy\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"omDHPeoJWjOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download it_core_news_lg"],"metadata":{"id":"a9N6P3La5n8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"it_core_news_lg\")"],"metadata":{"id":"n8BQEk_Owpt4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calcolo dell'IAA sulle annotazioni:\n","\n","1.   IAA sul GS Object\n","2.   IAA sul Defaulting"],"metadata":{"id":"N6DOzyGHvCfk"}},{"cell_type":"code","source":["#annotazione manuale\n","\n","file = pd.read_excel(r'/content/drive/MyDrive/IOcompletion_tesi_agnese_daffara/Dataset/IMPLICIT_results.xlsx')\n","predicted1 = file['obj_1'][:600]\n","predicted2 = file['obj_2'][:600]"],"metadata":{"id":"YbsKAg16TeAt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#cosine similarity tra annotatori sul GS Object\n","couples = zip(predicted1, predicted2)\n","\n","#definisco la funzione per calcolare la similarity\n","def calculate_similarity(word1, word2):\n","  word1_token = nlp(word1)\n","  word2_token = nlp(word2)\n","  if word2 == \"unknown\" or word1 == \"unknown\":\n","      return 0\n","  if word1_token.has_vector and word2_token.has_vector:\n","      similarity = word1_token.similarity(word2_token)\n","      return round(similarity,2)\n","  else:\n","      return 0\n","\n","similarity_scores = []\n","for couple in couples:\n","  similarity = calculate_similarity(couple[0], couple[1])\n","  similarity_scores.append(similarity)\n","\n","print('similarity tra annotatori =', similarity_scores)\n","print('media tra annotatori =', np.mean(similarity_scores))"],"metadata":{"id":"vGWnTXjoVAlh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calcolo dell'IAA su GS Object LEMMATIZZATI"],"metadata":{"id":"PBha3rEY2zQK"}},{"cell_type":"code","source":["#annotazione manuale lemmatizzata\n","\n","file = pd.read_excel(r'/content/drive/MyDrive/IOcompletion_tesi_agnese_daffara/Dataset/IMPLICIT_results_lemmatized.xlsx')\n","predicted1 = file['obj_1'][:600]\n","predicted2 = file['obj_2'][:600]"],"metadata":{"id":"p9Emz1aQ0ucJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#cosine similarity tra annotatori sul GS Object\n","couples = zip(predicted1, predicted2)\n","\n","#definisco la funzione per calcolare la similarity\n","def calculate_similarity(word1, word2):\n","  word1_token = nlp(word1)\n","  word2_token = nlp(word2)\n","  if word2 == \"unknown\" or word1 == \"unknown\":\n","      return 0\n","  if word1_token.has_vector and word2_token.has_vector:\n","      similarity = word1_token.similarity(word2_token)\n","      return round(similarity,2)\n","  else:\n","      return 0\n","\n","similarity_scores = []\n","for couple in couples:\n","  similarity = calculate_similarity(couple[0], couple[1])\n","  similarity_scores.append(similarity)\n","\n","print('similarity tra annotatori =', similarity_scores)\n","print('media tra annotatori =', np.mean(similarity_scores))"],"metadata":{"id":"88-mLcgR0ucX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import cohen_kappa_score\n","\n","#annotazione manuale del defaulting\n","def1 = file['defaulting_1'][:600]\n","def2 = file['defaulting_2'][:600]\n","\n","mapping = {\"Lexical Defaulting\": 0, \"Pragmatic Defaulting\": 1}\n","def1n = [mapping[a] for a in def1]\n","def2n = [mapping[a] for a in def2]\n","\n","cohen_kappa = cohen_kappa_score(def1n, def2n)\n","print(f'Cohen\\'s Kappa: {cohen_kappa}')"],"metadata":{"id":"1Qb65hoW4OLH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["verbi = ['ascoltare', 'attendere', 'bere', 'cantare', 'chiamare', 'combattere', 'condurre', 'consumare', 'correre', 'cucinare', 'dirigere', 'disegnare', 'fumare', 'giocare', 'guadagnare', 'guidare', 'leggere', 'mangiare', 'ordinare', 'pagare', 'perdere', 'pregare', 'preoccupare', 'provare', 'respirare', 'scrivere', 'servire', 'suonare', 'tirare', 'vincere']\n","gruppi1 = [def1[i:i+20] for i in range(0, len(def1), 20)]\n","gruppi2 = [def2[i:i+20] for i in range(0, len(def2), 20)]\n","\n","nuovo = zip(verbi, gruppi1, gruppi2) #ho creato un oggetto che associa ogni gruppo di 20 annotazioni del primo e del secondo annotatore ad un verbo\n","#nuovo = zip(verbi, gruppi2)\n","\n","dati = []\n","\n","for i in nuovo: #per ogni elemento nell'oggetto, conto quante volte Ã¨ stato annotato Lexical Defaulting e quante \"Pragmatic\"\n","  lde = 0\n","  pde = 0\n","  for p in i:\n","    for el in p:\n","      if \"Lexical Defaulting\" in el:\n","        lde = lde + 1\n","      if \"Pragmatic Defaulting\" in el:\n","        pde = pde + 1\n","  dato = (i[0], lde, pde)\n","  dati.append(dato)\n","\n","df = pd.DataFrame({'verbo': [dato[0] for dato in dati], 'Lexical Defaulting': [dato[1] for dato in dati], 'Pragmatic Defaulting': [dato[2] for dato in dati]})\n","print(df)\n","df = df.sort_values(by='Lexical Defaulting')\n","\n","df.set_index('verbo', inplace=True)\n","\n","#creo il grafico a barre empilato con Seaborn\n","sns.set(style='whitegrid', rc={\"axes.grid.axis\": \"y\", \"axes.grid.which\": \"both\", \"ytick.left\": True})\n","sns.set_context(\"notebook\", rc={\"ytick.major.pad\": 0.5})\n","sns.set(style='whitegrid')\n","ax = df.plot(kind='barh', stacked=True, figsize=(12, 8), colormap='viridis')\n","plt.title(\"Distribuzione del tipo di Defaulting tra i verbi\\n(IMPLICIT dataset: 2 annotatori)\")\n","#plt.title(\"Distribuzione del tipo di Defaulting tra i verbi\\n(IMPLICIT dataset: secondo annotatore)\")\n","plt.xlabel('n. annotazioni per tipo')\n","plt.ylabel('')\n","plt.legend(title='', loc='lower right', bbox_to_anchor=(1, 0.0), borderaxespad=0.)\n","plt.xlim(0, max(df.sum(axis=1)))\n","plt.xticks(range(1, 41))\n","#plt.xticks(range(1, 21))\n","plt.show()\n","\n","df"],"metadata":{"id":"NEqfukLg8exx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#calcolo il range di Oggetti per verbo nell'annotazione manuale\n","\n","verbi = ['ascoltare', 'attendere', 'bere', 'cantare', 'chiamare', 'combattere', 'condurre', 'consumare', 'correre', 'cucinare', 'dirigere', 'disegnare', 'fumare', 'giocare', 'guadagnare', 'guidare', 'leggere', 'mangiare', 'ordinare', 'pagare', 'perdere', 'pregare', 'preoccupare', 'provare', 'respirare', 'scrivere', 'servire', 'suonare', 'tirare', 'vincere']\n","\n","#creo una funzione che divide ogni lista in gruppi da 20 scores ciascuno, ogni gruppo si riferisce ad un verbo\n","def verb_values(score):\n","  gruppi = [score[i:i+20] for i in range(0, len(score), 20)]\n","  lista = list(zip(verbi, gruppi))\n","  return lista\n","\n","#creo una variabile che contiene le due liste di Oggetti GS\n","ress = [predicted1, predicted2]\n","#creo una lista che contiene tutti i risultati divisi in gruppi\n","results = []\n","for res in ress:\n","  results.append(verb_values(res))\n","#creo un dizionario che contiene i verbi e i punteggi che hanno ottenuto in ogni lista\n","d={}\n","for verbo in verbi:\n","  d[verbo] = []\n","#itero su ogni lista di risultati\n","for l in results:\n","  for item in l:\n","    #appendo alla value del verbo tutti gli score dei modelli per quel verbo\n","    d[item[0]].append(item[1])\n","\n","#definisco una funzione per contare le parole uniche per ogni verbo\n","def conta_parole_uniche(d):\n","    risultati = {}\n","    for verbo, liste_parole in d.items():\n","        parole_uniche = set()\n","        for serie in liste_parole:\n","            parole_uniche.update(serie)\n","        numero_parole_uniche = len(parole_uniche)\n","        risultati[verbo] = numero_parole_uniche\n","    return risultati\n","\n","#applico la funzione al dizionario\n","risultati = conta_parole_uniche(d)\n","\n","#creo il dataframe ordinato\n","df = pd.DataFrame(list(risultati.items()), columns=['Verbo', 'Range'])\n","df_sorted = df.sort_values(by='Range', ascending=False)\n","\n","#creo il grafico\n","sns.set_style(\"whitegrid\")\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='Range', y='Verbo', data=df_sorted, palette='viridis')\n","plt.xlabel('')\n","plt.ylabel('')\n","plt.title(\"Range di Oggetti (IMPLICIT dataset: 2 annotatori)\")\n","plt.xticks(range(0, int(df['Range'].max()) + 1, 1))\n","plt.show()"],"metadata":{"id":"EFfxf7dfEmfy"},"execution_count":null,"outputs":[]}]}